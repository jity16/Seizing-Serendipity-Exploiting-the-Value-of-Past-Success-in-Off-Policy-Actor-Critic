# Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic

<p align="center" style="font-size: 50px">
   <a href="https://arxiv.org/pdf/2306.02865">[Paper]</a>&emsp;<a href="https://jity16.github.io/BEE/">[Project Website]</a>
</p>


This repository is the official PyTorch implementation of **BAC**. **BAC** is an advanced, lightweight model-free reinforcement learning algorithm. BAC is based on an innovative Bellman operator BEE that addresses the value underestimation issue in the later stages of training by leveraging past successful experiences stored in the replay buffer. **BAC** demonstrates significant performance improvements across a wide range of continuous control tasks and real-world robot tasks.

<p align="center">
  <br><img src='bee.gif' width="1000"/><br>
</p>


# üõ†Ô∏è Installation Instructions

First, create a virtual environment and install all required packages. 
~~~
conda create -n bac python=3.8
pip install -r requirements.txt
~~~


## üíª Code Usage

If you would like to run BAC on a standard version of a certain `task`, please use `main.py` to train BAC policies.
~~~
python main.py --env_name task
~~~
If you would like to run BAC on a sparse reward version of a certain Meta-World `task`, please follow the command below.
~~~
python main.py --env_name task --reward_type sparse
~~~

## üìù Citation

If you use our method or code in your research, please consider citing the paper as follows:

```
@inproceedings{bee,
title={Seizing serendipity: Exploiting the value of past success in off-policy actor-critic},
author={Tianying Ji, Yu Luo, Fuchun Sun, Xianyuan Zhan, Jianwei Zhang, Huazhe Xu.},
booktitle={The Forty-first International Conference on Machine Learning},
year={2024},
url={https://arxiv.org/pdf/2306.02865}
}
```

## üôè Acknowledgement

BAC is licensed under the MIT license. MuJoCo and DeepMind Control Suite are licensed under the Apache 2.0 license. 
